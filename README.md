# Local AI WebUI

A fast, private, local-first WebUI for running local LLMs via Ollama and OpenAI-compatible APIs. Built for low-VRAM systems (RTX 3050 Ti / 4GB VRAM).

## Quick Start

### Prerequisites

- Python 3.11+
- Node.js 18+
- [Ollama](https://ollama.ai) installed and running (for local models)

### Backend Setup

```bash
# From the project root
pip install -r requirements.txt

# Copy and edit environment config
cp .env.example .env

# Start the backend server
python -m uvicorn backend.main:app --reload --port 8000
```

### Frontend Setup

```bash
cd frontend

# Install dependencies
npm install

# Start the dev server (proxies /api to backend)
npm run dev
```

Open **http://localhost:3000** in your browser.

### Using Ollama (Local)

1. Install Ollama from https://ollama.ai
2. Pull a model: `ollama pull llama3.2:3b` (or any model that fits your VRAM)
3. Ollama runs on `localhost:11434` by default â€” the app connects automatically

### Using a Remote Provider

1. Click **Provider** in the header
2. Switch to **OpenAI-Compatible**
3. Select a preset (OpenAI, DeepSeek, Groq, etc.) or enter a custom URL
4. Enter your API key
5. Click **Test Connection**
6. Select a model from the dropdown

## Features

- **Streaming chat** with markdown rendering (code blocks, tables, lists)
- **Web search grounding** â€” toggle the ğŸŒ Web button to augment LLM responses with live DuckDuckGo search results. Fetches top-5 results with full page extraction of the #1 result for richer context. Includes current date/time injection and system prompt grounding to reduce hallucination. No API key required.
- **Per-session system prompts** with built-in presets (Default, Concise, Technical)
- **Sliding window memory** with token-budget logic â€” adapts to conversation length
- **Two providers**: Ollama (local) and OpenAI-compatible (any remote service)
- **Model metadata display**: parameter count, quantization, context length
- **Performance controls**: context cap, max response tokens, low-VRAM safe mode
- **Session isolation**: each chat has its own prompt, history, and settings
- **Privacy indicator**: warns when messages leave your machine (remote provider)

## Architecture

```
backend/           FastAPI server (Python)
â”œâ”€â”€ main.py        Entry point, CORS, router mounting
â”œâ”€â”€ config.py      Settings, presets, defaults
â”œâ”€â”€ routers/       API endpoints (chat, sessions, models, providers)
â”œâ”€â”€ providers/     LLM provider abstraction (Ollama, OpenAI-compatible)
â”œâ”€â”€ sessions/      Session manager + sliding window memory
â””â”€â”€ utils/         Token estimation, web search

frontend/          React + Vite (JavaScript)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ App.jsx           Main shell
â”‚   â”œâ”€â”€ components/       UI components
â”‚   â”œâ”€â”€ hooks/            State management (useChat, useSession, useProvider)
â”‚   â”œâ”€â”€ services/api.js   Backend API client
â”‚   â””â”€â”€ utils/            Markdown config
```

## API Endpoints

| Method | Path | Description |
|--------|------|-------------|
| GET | `/api/health` | Health check |
| POST | `/api/chat/send` | Send message (SSE streaming) |
| GET | `/api/sessions` | List sessions |
| POST | `/api/sessions` | Create session |
| GET | `/api/sessions/:id` | Get session |
| PUT | `/api/sessions/:id` | Update session |
| DELETE | `/api/sessions/:id` | Delete session |
| POST | `/api/sessions/:id/clear` | Clear session memory |
| GET | `/api/models` | List models |
| GET | `/api/models/:id/info` | Model metadata |
| GET | `/api/providers/presets` | Provider presets |
| GET | `/api/providers/config` | Current config |
| POST | `/api/providers/test` | Test connection |
